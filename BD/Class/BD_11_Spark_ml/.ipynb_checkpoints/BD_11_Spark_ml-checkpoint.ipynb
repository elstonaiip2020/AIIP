{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"house.jpg\" alt=\"house\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will explore fitting a regression model and k-means clustering in Spark ML. We will be analysing household energy efficiency data. We explore different building shapes and how they differ with respect to the glazing area, the glazing area distribution, and the orientation, amongst other parameters. The dataset comprises 768 samples aiming to predict the heating and cooling load on these buildings. Data from [here](http://archive.ics.uci.edu/ml/datasets/Energy+efficiency)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start spark app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init('/opt/spark')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Spark_ml\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and clean data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = spark.read.csv(\"energy_data_ml.csv\",header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get rid of unwanted columns\n",
    "data = data.drop('_c0')\n",
    "#drop nas\n",
    "data = data.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# covert to numeric types\n",
    "#import double type from spark sql\n",
    "from pyspark.sql.types import DoubleType, IntegerType\n",
    "\n",
    "#convert all columns\n",
    "for col_name in data.columns:\n",
    "    data = data.withColumn(col_name, data[col_name].cast(DoubleType()))\n",
    "\n",
    "data = data.withColumn(\"Orientation\", data[\"Orientation\"].cast(IntegerType()))\n",
    "    \n",
    "# Split the data into training and test sets (30% held out for testing)\n",
    "(trainingData, testData) = data.randomSplit([0.7, 0.3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare data for model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import OneHotEncoder, VectorAssembler\n",
    "from pyspark.ml.regression import DecisionTreeRegressor\n",
    "from pyspark.ml.evaluation import RegressionEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform categorical variables to index\n",
    "labelEncoder = OneHotEncoder(inputCol=\"Orientation\", outputCol=\"OrientationInd\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assemble variables to one feature column\n",
    "assembler = VectorAssembler(\n",
    "    inputCols = ['Relative_Compactness',\"Surface_Area\",\"Wall_Area\",\"Roof_Area\",\"Overall_Height\",\"OrientationInd\",\"Glazing_Area\"],\n",
    "    outputCol = \"features\")\n",
    "\n",
    "#define the estimator - decision tree\n",
    "dt = DecisionTreeRegressor(labelCol=\"Cooling_Load\", featuresCol=\"features\")\n",
    "\n",
    "# Chain indexers and tree in a Pipeline\n",
    "pipeline = Pipeline(stages=[labelEncoder, assembler, dt])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit pipeline and transform data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fit the pipeline\n",
    "PipelineModel = pipeline.fit(trainingData)\n",
    "\n",
    "# transform using the pipeline\n",
    "predictions = PipelineModel.transform(testData)\n",
    "\n",
    "# evaluate model fit\n",
    "predictions.select(\"prediction\", \"Cooling_Load\")\n",
    "evaluator = RegressionEvaluator(\n",
    "    labelCol=\"Cooling_Load\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "rmse = evaluator.evaluate(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Root mean square error\n",
    "print(rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the fitted pipeline for later use\n",
    "PipelineModel.save(\"my_pipeline\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kmeans clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.clustering import KMeans\n",
    "\n",
    "# Trains a k-means model with 4 clusters.\n",
    "kmeans = KMeans(featuresCol='features', predictionCol='prediction',k=4)\n",
    "\n",
    "#transform data using pipeline\n",
    "pipeline = Pipeline(stages=[labelEncoder, assembler, kmeans])\n",
    "\n",
    "#fir pipeline\n",
    "PipelineModel = pipeline.fit(data)\n",
    "\n",
    "# transform using the pipeline\n",
    "predictions = PipelineModel.transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#view result\n",
    "predictions.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
