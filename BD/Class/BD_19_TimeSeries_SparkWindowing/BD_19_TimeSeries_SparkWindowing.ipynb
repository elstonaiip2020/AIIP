{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NwU28K5f1H3P"
   },
   "source": [
    "# Start a SparkSession\n",
    "This will start a local Spark session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "zgReRGl0y23D"
   },
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-5b8bde110bb1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mfindspark\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mfindspark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'/opt/spark'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# This is specific to the virtual lab. Use `findspark.init()`\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msql\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSparkSession\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mspark\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSparkSession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuilder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmaster\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"local[*]\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetOrCreate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\findspark.py\u001b[0m in \u001b[0;36minit\u001b[1;34m(spark_home, python_path, edit_rc, edit_profile)\u001b[0m\n\u001b[0;32m    133\u001b[0m     \u001b[1;31m# add pyspark to sys.path\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    134\u001b[0m     \u001b[0mspark_python\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mspark_home\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'python'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 135\u001b[1;33m     \u001b[0mpy4j\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mglob\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mspark_python\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'lib'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'py4j-*.zip'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    136\u001b[0m     \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mspark_python\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    137\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init('/opt/spark') # This is specific to the virtual lab. Use `findspark.init()`\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YSPVpQae_W8Q"
   },
   "source": [
    "# Reading and formatting dates\n",
    "\n",
    "You've already looked at reading files into spark. In here we will focus on reading data with dates and ensuring they are read in the right format.\n",
    "\n",
    "In this file, the most important columns to us are the   \n",
    "1) Date: the date of trade  \n",
    "2) Close: the value at the end of the day  \n",
    "\n",
    "This read puts the data directly into a dataframe.  \n",
    "Data source:  https://in.finance.yahoo.com/q/hp?s=AAPL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 394
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1604,
     "status": "ok",
     "timestamp": 1524784965692,
     "user": {
      "displayName": "Eyram Adjaku",
      "photoUrl": "//lh3.googleusercontent.com/-5iFk2YnMvn8/AAAAAAAAAAI/AAAAAAAAAKU/-AxYB6DNJXE/s50-c-k-no/photo.jpg",
      "userId": "113279431062694058728"
     },
     "user_tz": 0
    },
    "id": "F7lyDTRG_X76",
    "outputId": "1edaca26-a25e-450d-edcf-ff946efa9167"
   },
   "outputs": [],
   "source": [
    "# read csv file\n",
    "dfstock2011 = spark.read.csv('aapl.csv',header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfstock2011.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 394
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1604,
     "status": "ok",
     "timestamp": 1524784965692,
     "user": {
      "displayName": "Eyram Adjaku",
      "photoUrl": "//lh3.googleusercontent.com/-5iFk2YnMvn8/AAAAAAAAAAI/AAAAAAAAAKU/-AxYB6DNJXE/s50-c-k-no/photo.jpg",
      "userId": "113279431062694058728"
     },
     "user_tz": 0
    },
    "id": "F7lyDTRG_X76",
    "outputId": "1edaca26-a25e-450d-edcf-ff946efa9167"
   },
   "outputs": [],
   "source": [
    "# select only \"date\" and \"close\" column\n",
    "dfstock2011 = dfstock2011.select('Date','Close')\n",
    "\n",
    "# inspect the first 10 rows\n",
    "dfstock2011.show(10)\n",
    "\n",
    "\n",
    "# the printSchema() method tells you the data type of each column\n",
    "dfstock2011.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BrnyLOgP2xVA"
   },
   "source": [
    "From the above we see that the Date and close columns are formatted as strings.\n",
    "\n",
    "We will need to tell spark what types they are. `Close` should be an int and `Date` should be Date object.\n",
    "\n",
    "Just like we formatted dates in pandas spark allows us to similar.\n",
    "\n",
    "Spark has the function `to_timestamp` which is used to format Datetime objects, eg. \"*2018-02-02 18:43:00*\" and `to_date` which is used for only dates, eg. \"*2018-02-02*\". Its also capable of converting epoch dates.\n",
    "\n",
    "Our data is only a date so we use the `to_date` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 394
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 830,
     "status": "ok",
     "timestamp": 1524779064826,
     "user": {
      "displayName": "Eyram Adjaku",
      "photoUrl": "//lh3.googleusercontent.com/-5iFk2YnMvn8/AAAAAAAAAAI/AAAAAAAAAKU/-AxYB6DNJXE/s50-c-k-no/photo.jpg",
      "userId": "113279431062694058728"
     },
     "user_tz": 0
    },
    "id": "slkNosrm1HyW",
    "outputId": "d134bdd7-d1c0-4179-cf99-0626c175db43"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import datediff, to_date\n",
    "\n",
    "#using the to_date function we convert the string to a date object also providing it the write format.\n",
    "dfstock2011 = dfstock2011.withColumn(\"Date\",to_date(dfstock2011[\"Date\"],'yyyy-MM-dd'))\n",
    "\n",
    "# while at it also convert the close column to a float\n",
    "dfstock2011 = dfstock2011.withColumn(\"Close\",dfstock2011[\"Close\"].cast('float'))\n",
    "\n",
    "dfstock2011.show(10)\n",
    "dfstock2011.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cGZpRZpl6ylw"
   },
   "source": [
    "Now viewing the schema of the dataframe we have them in the write types.\n",
    "\n",
    "To convert epoch dates it provide the function `from_unixtime`\n",
    "\n",
    "In the example below we create a dummy dataframe with epoch dates and convert it a timestamp object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 319
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 912,
     "status": "ok",
     "timestamp": 1524779982068,
     "user": {
      "displayName": "Eyram Adjaku",
      "photoUrl": "//lh3.googleusercontent.com/-5iFk2YnMvn8/AAAAAAAAAAI/AAAAAAAAAKU/-AxYB6DNJXE/s50-c-k-no/photo.jpg",
      "userId": "113279431062694058728"
     },
     "user_tz": 0
    },
    "id": "GizqmKE27gqm",
    "outputId": "36720bde-4d44-41a4-f595-a0443414f1de"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import unix_timestamp\n",
    "from pyspark.sql.functions import from_unixtime\n",
    "\n",
    "df = spark.createDataFrame([(\"1524460623\",), (\"1524560699\",), (\"151456057\",)], ['date_str'])\n",
    "df.show(10)\n",
    "\n",
    "df2 = df.select('date_str', from_unixtime('date_str').alias('timestamp'))\n",
    "\n",
    "df2.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SXthbZbnJVQC"
   },
   "source": [
    "# Date Formatting\n",
    "Spark also allows us to reformat a date or timestamp column to another date format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 846,
     "status": "ok",
     "timestamp": 1524780364324,
     "user": {
      "displayName": "Eyram Adjaku",
      "photoUrl": "//lh3.googleusercontent.com/-5iFk2YnMvn8/AAAAAAAAAAI/AAAAAAAAAKU/-AxYB6DNJXE/s50-c-k-no/photo.jpg",
      "userId": "113279431062694058728"
     },
     "user_tz": 0
    },
    "id": "XuTzjX5Ph1BJ",
    "outputId": "2093ba44-93e0-4c7f-da7c-0a97d725d7af"
   },
   "outputs": [],
   "source": [
    "# the time 00:00:00 are of no use to us. \n",
    "# Suppose we wanted to formatted the timestamp to only show the date in a format we like\n",
    "from pyspark.sql.functions import date_format\n",
    "\n",
    "dfstock2011.select(date_format('Date', \"dd-MMM-yyy\").alias(\"date_type1\")).show(5)\n",
    "\n",
    "dfstock2011.select(date_format('Date', \"dd/MM/yy\").alias(\"date_type2\")).show(5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KcXNttVB_m4M"
   },
   "source": [
    "# Subtracting Dates\n",
    "\n",
    "Just as we were able to subtract two dates we can do same with spark. To that we use the function `datediff(end,start)`\n",
    "\n",
    "See documentation [here](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.functions.*datediff*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 319
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 742,
     "status": "ok",
     "timestamp": 1524793591122,
     "user": {
      "displayName": "Eyram Adjaku",
      "photoUrl": "//lh3.googleusercontent.com/-5iFk2YnMvn8/AAAAAAAAAAI/AAAAAAAAAKU/-AxYB6DNJXE/s50-c-k-no/photo.jpg",
      "userId": "113279431062694058728"
     },
     "user_tz": 0
    },
    "id": "OqahmW_jARjK",
    "outputId": "df672e53-769d-4a4c-f140-e49784724f4d"
   },
   "outputs": [],
   "source": [
    "#create dummy dataframe\n",
    "df = spark.createDataFrame(\n",
    "    [(\"2005-11-23\",\"2005-11-23\"),\n",
    "     (\"2010-10-25\",\"2010-12-31\"), \n",
    "     (\"2009-06-30\",\"2010-06-30\")], \n",
    "    ['start_date','end_date'])\n",
    "\n",
    "# convert dates to date object\n",
    "df = df.withColumn(\"start_date\",to_date(df[\"start_date\"],'yyyy-MM-dd'))\n",
    "df = df.withColumn(\"end_date\",  to_date(df[\"end_date\"],  'yyyy-MM-dd'))\n",
    "df.show(10)\n",
    "\n",
    "\n",
    "# using the date_diff functiion to find the difference between the two dates\n",
    "df.withColumn(\"difference_in_days\",datediff(df.end_date,df.start_date)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5NhL4SJwNSds"
   },
   "source": [
    "# Adding to and subtracting from dates\n",
    "\n",
    "We can add days,months and years to dates in spark. For this we use \n",
    "\n",
    "\n",
    "*   **Adding** - `date_add(start,days)`\n",
    "*   **Subtracting** - `date_sub(start,days)`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 319
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 836,
     "status": "ok",
     "timestamp": 1524784445552,
     "user": {
      "displayName": "Eyram Adjaku",
      "photoUrl": "//lh3.googleusercontent.com/-5iFk2YnMvn8/AAAAAAAAAAI/AAAAAAAAAKU/-AxYB6DNJXE/s50-c-k-no/photo.jpg",
      "userId": "113279431062694058728"
     },
     "user_tz": 0
    },
    "id": "0nr03nHzNbE4",
    "outputId": "7066727c-fee8-477f-c6c7-fd54ef21900e"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import to_date,date_add,date_sub\n",
    "#create dummy dataframe\n",
    "df = spark.createDataFrame(\n",
    "    [(\"2005-11-23\",),\n",
    "     (\"2010-10-25\",), \n",
    "     (\"2009-06-30\",)], \n",
    "    ['date',])\n",
    "\n",
    "# convert dates to date object\n",
    "df = df.withColumn(\"date\",to_date(df[\"date\"],'yyyy-MM-dd'))\n",
    "df.show(10)\n",
    "\n",
    "df = df.withColumn(\"plus_10_days\",date_add(df[\"date\"],10))\n",
    "df = df.withColumn(\"minus_60_days\",date_sub(df[\"date\"],60))\n",
    "df.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hMEg3vLvzKC0"
   },
   "source": [
    "# Windowing in Spark\n",
    "\n",
    "\n",
    "\n",
    "Read more about spark windowing function https://databricks.com/blog/2015/07/15/introducing-window-functions-in-spark-sql.html\n",
    "\n",
    "You can also find the official documentation [here](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.Window)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 508
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1652,
     "status": "ok",
     "timestamp": 1524785694232,
     "user": {
      "displayName": "Eyram Adjaku",
      "photoUrl": "//lh3.googleusercontent.com/-5iFk2YnMvn8/AAAAAAAAAAI/AAAAAAAAAKU/-AxYB6DNJXE/s50-c-k-no/photo.jpg",
      "userId": "113279431062694058728"
     },
     "user_tz": 0
    },
    "id": "wWp4tCuSzAYR",
    "outputId": "fdec9479-8849-4ce0-c09d-6baeee0f7db5"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import count, round\n",
    "\n",
    "windowSpec = Window.partitionBy(dfstock2011[\"Close\"]).orderBy(dfstock2011.Date.asc())\n",
    "\n",
    "counts = count(dfstock2011.Close).over(windowSpec)\n",
    "\n",
    "dfstock2011.select(round(dfstock2011.Close, 0), counts.alias(\"Counts\")).show(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7GbPU-YFoFc2"
   },
   "source": [
    "Using the same understaning of windowing from pandas we are able to do same in spark.\n",
    "\n",
    "With spark you first create a window, then create columns of that do calculations over the window, then you can append it to your original data.\n",
    "\n",
    "In the example below we want to find the moving average and cummlative sum of the \"close\" values\n",
    "\n",
    "\n",
    "Read more about spark windowing function https://databricks.com/blog/2015/07/15/introducing-window-functions-in-spark-sql.html\n",
    "\n",
    "You can also find the official documentation [here](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.Window)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 319
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 734,
     "status": "ok",
     "timestamp": 1524793977130,
     "user": {
      "displayName": "Eyram Adjaku",
      "photoUrl": "//lh3.googleusercontent.com/-5iFk2YnMvn8/AAAAAAAAAAI/AAAAAAAAAKU/-AxYB6DNJXE/s50-c-k-no/photo.jpg",
      "userId": "113279431062694058728"
     },
     "user_tz": 0
    },
    "id": "U5ewqx2ZSf53",
    "outputId": "cbe87470-1ea1-474d-b094-7c3b6d8fbb0c"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import  round,avg,sum\n",
    "\n",
    "# create window on data ordery by date\n",
    "windowSpec = Window.orderBy(dfstock2011.Date.asc())\n",
    "\n",
    "# calculate average and sum over the window\n",
    "avgs = avg(dfstock2011.Close).over(windowSpec)\n",
    "sums = sum(dfstock2011.Close).over(windowSpec)\n",
    "\n",
    "# add calculated columns to orignal data and show\n",
    "dfstock2011.withColumn(\"moving_average\",round(avgs,2)).withColumn(\"cummlative_sum\",round(sums,2)).show(10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8Np9TxB_nqog"
   },
   "source": [
    "Now if we wanted to find the monthly moving average and cumulative  sum we need create a column holding just the month of the year and create a partition in our window using the month column. This makes sure all calculations done on the window are done on partition in isolation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 319
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1108,
     "status": "ok",
     "timestamp": 1524794043850,
     "user": {
      "displayName": "Eyram Adjaku",
      "photoUrl": "//lh3.googleusercontent.com/-5iFk2YnMvn8/AAAAAAAAAAI/AAAAAAAAAKU/-AxYB6DNJXE/s50-c-k-no/photo.jpg",
      "userId": "113279431062694058728"
     },
     "user_tz": 0
    },
    "id": "HAFAYBIVlGE6",
    "outputId": "2873ab7b-235b-4468-c033-1c2d9caf8d75"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import round,avg,sum,month\n",
    "\n",
    "# create column to hold month of the year\n",
    "dfstock2011 = dfstock2011.withColumn(\"month\",month(dfstock2011.Date))\n",
    "\n",
    "# create and partition window by month and order by date\n",
    "windowSpec = Window.partitionBy(dfstock2011.month).orderBy(dfstock2011.Date.asc())\n",
    "\n",
    "# calculate average and sum over the window\n",
    "avgs = avg(dfstock2011.Close).over(windowSpec)\n",
    "sums = sum(dfstock2011.Close).over(windowSpec)\n",
    "\n",
    "# add calculated columns to orignal data and show\n",
    "dfstock2011.withColumn(\"moving_average\",round(avgs,2)).withColumn(\"cummlative_sum\",round(sums,2)).show(10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NBn_8xIIpEG0"
   },
   "source": [
    "# Assessment\n",
    "\n",
    "Using the data in internet_traffic.csv provide the following statistics\n",
    "\n",
    "* Average houly bits transferred\n",
    "* Total daily bits transferred\n",
    "\n",
    "The data shows in megabits the amount of data transferred by an ISP at 5 minute intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 508
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1938,
     "status": "ok",
     "timestamp": 1524793080278,
     "user": {
      "displayName": "Eyram Adjaku",
      "photoUrl": "//lh3.googleusercontent.com/-5iFk2YnMvn8/AAAAAAAAAAI/AAAAAAAAAKU/-AxYB6DNJXE/s50-c-k-no/photo.jpg",
      "userId": "113279431062694058728"
     },
     "user_tz": 0
    },
    "id": "efMq6NmspVmm",
    "outputId": "236c0cbb-c5fb-4d5d-ab25-991b4308aabc"
   },
   "outputs": [],
   "source": [
    "# Write code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "RuWAF8mWWzG4",
    "SXthbZbnJVQC"
   ],
   "default_view": {},
   "name": "Spark on Colaboratory.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
